<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Spark Streamin by keshavkakarla</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-dark.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/main.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1>Spark Streamin</h1>
        <p>Apache Spark Streamin</p>
      </header>

      <div id="banner">
        <span id="logo"></span>

        <a href="https://github.com/keshavkakarla/spark" class="button fork"><strong>View On GitHub</strong></a>
        <div class="downloads">
          <span>Downloads:</span>
          <ul>
            <li><a href="https://github.com/keshavkakarla/spark/zipball/master" class="button">ZIP</a></li>
            <li><a href="https://github.com/keshavkakarla/spark/tarball/master" class="button">TAR</a></li>
          </ul>
        </div>
      </div><!-- end banner -->

    <div class="wrapper">
      <nav>
        <ul></ul>
      </nav>
      <section>
        <h1>
<a id="apache-spark" class="anchor" href="#apache-spark" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Apache Spark</h1>

<p>Spark is a fast and general cluster computing system for Big Data. It provides
high-level APIs in Scala, Java, Python, and R, and an optimized engine that
supports general computation graphs for data analysis. It also supports a
rich set of higher-level tools including Spark SQL for SQL and DataFrames,
MLlib for machine learning, GraphX for graph processing,
and Spark Streaming for stream processing.</p>

<p><a href="http://spark.apache.org/">http://spark.apache.org/</a></p>

<h2>
<a id="online-documentation" class="anchor" href="#online-documentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Online Documentation</h2>

<p>You can find the latest Spark documentation, including a programming
guide, on the <a href="http://spark.apache.org/documentation.html">project web page</a>
and <a href="https://cwiki.apache.org/confluence/display/SPARK">project wiki</a>.
This README file only contains basic setup instructions.</p>

<h2>
<a id="building-spark" class="anchor" href="#building-spark" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building Spark</h2>

<p>Spark is built using <a href="http://maven.apache.org/">Apache Maven</a>.
To build Spark and its example programs, run:</p>

<pre><code>build/mvn -DskipTests clean package
</code></pre>

<p>(You do not need to do this if you downloaded a pre-built package.)
More detailed documentation is available from the project site, at
<a href="http://spark.apache.org/docs/latest/building-spark.html">"Building Spark"</a>.
For developing Spark using an IDE, see <a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse">Eclipse</a>
and <a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ">IntelliJ</a>.</p>

<h2>
<a id="interactive-scala-shell" class="anchor" href="#interactive-scala-shell" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interactive Scala Shell</h2>

<p>The easiest way to start using Spark is through the Scala shell:</p>

<pre><code>./bin/spark-shell
</code></pre>

<p>Try the following command, which should return 1000:</p>

<pre><code>scala&gt; sc.parallelize(1 to 1000).count()
</code></pre>

<h2>
<a id="interactive-python-shell" class="anchor" href="#interactive-python-shell" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interactive Python Shell</h2>

<p>Alternatively, if you prefer Python, you can use the Python shell:</p>

<pre><code>./bin/pyspark
</code></pre>

<p>And run the following command, which should also return 1000:</p>

<pre><code>&gt;&gt;&gt; sc.parallelize(range(1000)).count()
</code></pre>

<h2>
<a id="example-programs" class="anchor" href="#example-programs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example Programs</h2>

<p>Spark also comes with several sample programs in the <code>examples</code> directory.
To run one of them, use <code>./bin/run-example &lt;class&gt; [params]</code>. For example:</p>

<pre><code>./bin/run-example SparkPi
</code></pre>

<p>will run the Pi example locally.</p>

<p>You can set the MASTER environment variable when running examples to submit
examples to a cluster. This can be a mesos:// or spark:// URL,
"yarn" to run on YARN, and "local" to run
locally with one thread, or "local[N]" to run locally with N threads. You
can also use an abbreviated class name if the class is in the <code>examples</code>
package. For instance:</p>

<pre><code>MASTER=spark://host:7077 ./bin/run-example SparkPi
</code></pre>

<p>Many of the example programs print usage help if no params are given.</p>

<h2>
<a id="running-tests" class="anchor" href="#running-tests" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running Tests</h2>

<p>Testing first requires <a href="#building-spark">building Spark</a>. Once Spark is built, tests
can be run using:</p>

<pre><code>./dev/run-tests
</code></pre>

<p>Please see the guidance on how to
<a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools">run tests for a module, or individual tests</a>.</p>

<h2>
<a id="a-note-about-hadoop-versions" class="anchor" href="#a-note-about-hadoop-versions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A Note About Hadoop Versions</h2>

<p>Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported
storage systems. Because the protocols have changed in different versions of
Hadoop, you must build Spark against the same version that your cluster runs.</p>

<p>Please refer to the build documentation at
<a href="http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version">"Specifying the Hadoop Version"</a>
for detailed guidance on building for a particular distribution of Hadoop, including
building for particular Hive and Hive Thriftserver distributions.</p>

<h2>
<a id="configuration" class="anchor" href="#configuration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h2>

<p>Please refer to the <a href="http://spark.apache.org/docs/latest/configuration.html">Configuration Guide</a>
in the online documentation for an overview on how to configure Spark.</p>
      </section>
      <footer>
        <p>Project maintained by <a href="https://github.com/keshavkakarla">keshavkakarla</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>
